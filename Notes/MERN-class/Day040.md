# MERN Class Notes  
**Date:** 21 Jan 2026 (Wednesday)  
**Topic:** Introduction to LLMs, Prompt Engineering, and Google Generative AI SDK

---

## 1. What is an LLM?
**LLM (Large Language Model)** refers to AI models trained on massive amounts of text data to understand and generate human-like language.

Key characteristics:
- Trained on **billions/trillions of tokens**
- Can perform:
  - Text generation
  - Code completion
  - Summarization
  - Translation
  - Question answering
- Examples:
  - GPT models
  - Gemini
  - Claude
  - LLaMA

Think of an LLM as a **probability engine for language**—it doesn’t “know” things, it predicts what *should come next* based on patterns learned from data.

---

## 2. Prompt Engineering
**Prompt engineering** is the practice of structuring inputs (prompts) to guide an LLM toward better, more accurate, and more useful outputs.

Why it matters:
- Same model + different prompt = wildly different results
- The model is powerful, but *directionless without constraints*

Core techniques:
- **Role prompting**  
  _“You are a senior MERN stack developer…”_
- **Context injection**  
  Provide background or assumptions
- **Instruction clarity**  
  Explicit steps > vague requests
- **Output formatting constraints**  
  Markdown, JSON, bullet points, etc.

In short:  
> Prompt engineering is not magic—it's **clear thinking written down**.

---

## 3. NLP vs LLM (Important Distinction)

### NLP (Natural Language Processing)
Traditional NLP is:
- Rule-based or statistically driven
- Task-specific
- Requires feature engineering

Examples:
- Tokenization
- POS tagging
- Named Entity Recognition
- Sentiment analysis using classical ML

### LLMs
LLMs are:
- End-to-end trained
- Context-aware
- General-purpose
- Less brittle

Key difference:
- **NLP breaks language into parts**
- **LLMs model language as a whole**

You can think of it like this:
> NLP is a toolkit.  
> LLMs are a **brain trained on language**.

---

## 4. Google Generative AI (Gemini) with SDK
Google provides access to its generative models (Gemini) using an official SDK.

### Installation (Node.js)
```bash
npm install @ai-sdk/google
```

## 9. Making a Direct API Call to Gemini using REST (Postman / curl style)

This example demonstrates how to directly call the **Google Generative Language API (Gemini)** using a raw HTTP request.
Useful for:
- Testing models quickly
- Understanding request/response structure
- Debugging before SDK integration

---

### API Endpoint
```

[https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent](https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent)

```

> Note: The endpoint targets the **Gemini 3 Flash Preview** model, optimized for speed and lightweight responses.

---

### Headers
When making the request (via Postman, curl, or similar), include the following headers:

| Key           | Value              |
|---            |---                 |
| x-goog-api-key| `YOUR_API_KEY_HERE`|
| Content-Type  | `application/json` |

⚠️ **Security Reminder**  
- Never expose API keys in frontend code
- Store keys in `.env` files or server-side environment variables
- API calls should be routed through a backend service

---

### Request Body (JSON)
Set the body type to **raw → JSON**, and use the following structure:

```json
{
  "contents": [
    {
      "parts": [
        {
          "text": "Explain quantum entanglement briefly."
        }
      ]
    }
  ]
}
```

---

### Explanation of Request Structure

* **contents**
  Represents the full input sent to the model.

* **parts**
  Allows multiple input segments (text, images, etc.).

* **text**
  The actual prompt provided to the LLM.

This structure allows for future extensibility, such as:

* Multi-turn conversations
* Multimodal inputs (text + image)
* Tool calling

---

### When to Use REST Instead of SDK

Use direct REST calls when:

* Learning how the API works internally
* Testing endpoints quickly
* Working in environments without Node.js
* Debugging low-level request issues

Use SDKs when:

* Building production apps
* Managing retries, streaming, or abstraction layers
* Integrating with frameworks like Express or Next.js

---

### Architectural Best Practice

```
Client (Browser / App)
        ↓
Backend Server (Node / Express)
        ↓
Gemini API (with API key)
```

Calling Gemini directly from the client is **unsafe** and **costly** in the long run.

---

**Key Insight:**
LLMs are easy to call.
Building them **correctly** is where engineering still matters.




